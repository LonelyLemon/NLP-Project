{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e436780",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft datasets accelerate torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad33981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/admin/.local/lib/python3.10/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abce1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ee137",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "MAX_LENGTH = 256\n",
    "SRC = 'en'\n",
    "\n",
    "TRAIN_EN_PATH = ''\n",
    "TRAIN_VI_PATH = ''\n",
    "TEST_EN_PATH = ''\n",
    "TEST_VI_PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c23ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_EN_PATH, 'r', encoding='utf-8') as f:\n",
    "    ens = f.read().splitlines()\n",
    "with open(TRAIN_VI_PATH, 'r', encoding='utf-8') as f:\n",
    "    vis = f.read().splitlines()\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "indices = list(range(len(ens)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "split = int(len(ens) * 0.1)\n",
    "valid_indices = indices[:split]\n",
    "train_indices = indices[split:]\n",
    "\n",
    "train_ens = [ens[i] for i in train_indices]\n",
    "train_vis = [vis[i] for i in train_indices]\n",
    "\n",
    "valid_ens = [ens[i] for i in valid_indices]\n",
    "valid_vis = [vis[i] for i in valid_indices]\n",
    "\n",
    "if SRC == 'en':\n",
    "    train_src_texts = train_ens\n",
    "    train_tgt_texts = train_vis\n",
    "    valid_src_texts = valid_ens\n",
    "    valid_tgt_texts = valid_vis\n",
    "\n",
    "    train_src_texts = [\n",
    "        f\"Translate the following English medical text into Vietnamese exactly, preserving all medical terms, numbers, units, and formatting: {s}\" \n",
    "        for s in train_ens\n",
    "    ]\n",
    "    valid_src_texts = [\n",
    "        f\"Translate the following English medical text into Vietnamese exactly, preserving all medical terms, numbers, units, and formatting: {s}\" \n",
    "        for s in valid_ens\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    train_src_texts = train_vis\n",
    "    train_tgt_texts = train_ens\n",
    "    valid_src_texts = valid_vis\n",
    "    valid_tgt_texts = valid_ens\n",
    "\n",
    "    train_src_texts = [\n",
    "        f\"Dịch chính xác đoạn văn y tế sau từ Vietnamese sang English, giữ nguyên tất cả thuật ngữ, con số, đơn vị và định dạng: {s}\" \n",
    "        for s in train_vis\n",
    "    ]\n",
    "    valid_src_texts = [\n",
    "        f\"Dịch chính xác đoạn văn y tế sau từ Vietnamese sang English, giữ nguyên tất cả thuật ngữ, con số, đơn vị và định dạng: {s}\" \n",
    "        for s in valid_vis\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67656305",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({\"input\": train_src_texts, \"output\": train_tgt_texts})\n",
    "valid_data = Dataset.from_dict({\"input\": valid_src_texts, \"output\": valid_tgt_texts})\n",
    "def tokenize_fn(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "    dec = tokenizer(\n",
    "        batch[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "    enc[\"labels\"] = dec[\"input_ids\"]\n",
    "    return enc\n",
    "\n",
    "train_ds = train_data.map(tokenize_fn, batched=True, remove_columns=[\"input\",\"output\"])\n",
    "valid_ds = valid_data.map(tokenize_fn, batched=True, remove_columns=[\"input\",\"output\"])\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c153b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./qwen-lora-ft-{SRC}\",\n",
    "\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=1,\n",
    "    \n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=500,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    num_train_epochs=10,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    log_level=\"info\",\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=None,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    greater_is_better=False,\n",
    "\n",
    "    fp16=True,\n",
    "    dataloader_drop_last=False,\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    seed=42,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.train()\n",
    "model.save_pretrained(f\"./qwen-lora-ft-{SRC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e7622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    replace_map = {\n",
    "        '–':'-', '—':'-', '−':'-', '﹣':'-',\n",
    "        '':'≥', '':'≤',\n",
    "        '∕':'/', '／':'/',\n",
    "        '＝':'=',\n",
    "        '＋':'+',\n",
    "        '％':'%',\n",
    "        '±':'±',\n",
    "        '‘':\"'\", '’':\"'\",\n",
    "        '“':'\"', '”':'\"',\n",
    "    }\n",
    "    for k, v in replace_map.items():\n",
    "        text = text.replace(k, v)\n",
    "    text = re.sub(r'\\s+%', '%', text)\n",
    "    text = re.sub(r'\\s*/\\s*', '/', text)\n",
    "    text = re.sub(r'\\s*-\\s*', '-', text)\n",
    "    for symbol in ['<', '>', '=', '±', '+', '≥', '≤']:\n",
    "        text = re.sub(r'\\s*{}\\s*'.format(re.escape(symbol)), f' {symbol} ', text)\n",
    "\n",
    "    def merge_numbers_only(match):\n",
    "        s = match.group(0)\n",
    "        return s.replace(' ', '')\n",
    "    text = re.sub(r'\\b\\d[\\d\\s/.,%]*\\d\\b', merge_numbers_only, text)\n",
    "\n",
    "    text = re.sub(r'([.,:?!])(?=[A-Za-zÀ-ÖØ-öø-ÿ])', r'\\1 ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'(\\d),\\s*(\\d)', r'\\1,\\2', text)\n",
    "    text = re.sub(r'(\\d)([.,])\\s*(\\d)', r'\\1\\2\\3', text)\n",
    "    return text\n",
    "\n",
    "with open(TEST_EN_PATH, 'r', encoding='utf-8') as f:\n",
    "    test_ens = f.read().splitlines()\n",
    "with open(TEST_VI_PATH, 'r', encoding='utf-8') as f:\n",
    "    test_vis = f.read().splitlines()\n",
    "\n",
    "if SRC == 'en':\n",
    "    test_src_texts = test_ens\n",
    "    test_tgt_texts = test_vis\n",
    "    test_src_texts = [normalize(text) for text in test_src_texts]\n",
    "    test_src_texts = [\n",
    "        f\"Translate the following English medical text into Vietnamese exactly, preserving all medical terms, numbers, units, and formatting: {s}\" \n",
    "        for s in test_ens\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    test_src_texts = test_vis\n",
    "    test_tgt_texts = test_ens\n",
    "    test_src_texts = [normalize(text) for text in test_src_texts]\n",
    "    test_src_texts = [\n",
    "        f\"Dịch chính xác đoạn văn y tế sau từ Vietnamese sang English, giữ nguyên tất cả thuật ngữ, con số, đơn vị và định dạng: {s}\" \n",
    "        for s in test_vis\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "preds = []\n",
    "for src_text in tqdm(test_src_texts):\n",
    "    inputs = tokenizer(\n",
    "        src_text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_LENGTH,\n",
    "            do_sample=False\n",
    "        )\n",
    "    pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    preds.append(pred)\n",
    "\n",
    "with open(f'predictions_{SRC}.txt', 'w', encoding='utf-8') as f:\n",
    "    for pred in preds:\n",
    "        f.write(pred + '\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acacbb5c",
   "metadata": {},
   "source": [
    "# Config tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b7983",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 30\n",
    "WARMUP_STEPS = 256\n",
    "\n",
    "MODEL_DIM = 384\n",
    "NUM_HEADS = 6\n",
    "NUM_ENC_LAYERS = 6\n",
    "NUM_DEC_LAYERS = 6\n",
    "DROPOUT = 0.15\n",
    "MAX_LEN_EN = 150\n",
    "MAX_LEN_VI = 180\n",
    "\n",
    "USE_ROPE = False\n",
    "USE_SUBWORD = True\n",
    "VOCAB_SIZE_EN = 12000\n",
    "VOCAB_SIZE_VI = 7000\n",
    "VOCAB_MODEL_TYPE = 'unigram'\n",
    "\n",
    "SRC = 'en'\n",
    "TRG = 'vi'\n",
    "\n",
    "NUM_WORKERS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5b947",
   "metadata": {},
   "source": [
    "# Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "    torch \\\n",
    "    torchvision \\\n",
    "    torchaudio \\\n",
    "    datasets \\\n",
    "    sentencepiece \\\n",
    "    sacrebleu \\\n",
    "    rouge-score \\\n",
    "    tqdm \\\n",
    "    numpy \\\n",
    "    matplotlib \\\n",
    "    seaborn\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sacrebleu.metrics import BLEU\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from typing import Literal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import html\n",
    "import unicodedata\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.register_buffer(\n",
    "            'inv_freq',\n",
    "            1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        device = x.device\n",
    "        positions = torch.arange(L, device=device).float()\n",
    "        theta = torch.einsum('n,d->nd', positions, self.inv_freq)\n",
    "        cos = theta.cos()[None, :, :]\n",
    "        sin = theta.sin()[None, :, :]\n",
    "        x_reshaped = x.view(B, L, D//2, 2)\n",
    "        x_even = x_reshaped[...,0]\n",
    "        x_odd  = x_reshaped[...,1]\n",
    "        cos = cos.expand(B, -1, -1)\n",
    "        sin = sin.expand(B, -1, -1)\n",
    "        x_rot_even = x_even * cos - x_odd * sin\n",
    "        x_rot_odd  = x_even * sin + x_odd * cos\n",
    "        x_rot = torch.stack([x_rot_even, x_rot_odd], dim=-1).flatten(-2)\n",
    "        return x_rot\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        max_len: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_dim: int, \n",
    "            hidden_dim: int = 2048, \n",
    "            dropout: float = 0.1\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(model_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.activation(self.linear1(x))\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.linear2(hidden)\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_dim: int, \n",
    "        num_heads: int, \n",
    "        dropout: float = 0.1,\n",
    "        use_rope: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert model_dim % num_heads == 0, \"embedding_dim ph·∫£i chia h·∫øt cho num_heads\"\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = model_dim // num_heads\n",
    "        self.use_rope = use_rope\n",
    "\n",
    "        if use_rope:\n",
    "            self.rope = RoPE(self.head_dim)\n",
    "\n",
    "        self.Q_linear = nn.Linear(model_dim, model_dim)\n",
    "        self.K_linear = nn.Linear(model_dim, model_dim)\n",
    "        self.V_linear = nn.Linear(model_dim, model_dim)\n",
    "        self.out_proj = nn.Linear(model_dim, model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q: [B, L_q, D]\n",
    "            k: [B, L_k, D]\n",
    "            v: [B, L_v, D]\n",
    "        Returns:\n",
    "            context: [B, L_q, D]\n",
    "        \"\"\"\n",
    "        B = q.size(0)\n",
    "        L_q = q.size(1)\n",
    "        L_k = k.size(1)\n",
    "        L_v = v.size(1)\n",
    "\n",
    "        # Linear projections: [B, L, D]\n",
    "        Q = self.Q_linear(q)\n",
    "        K = self.K_linear(k)\n",
    "        V = self.V_linear(v)\n",
    "\n",
    "        # Split heads: [B, L, D] -> [B, num_heads, L, head_dim]\n",
    "        Q = Q.view(B, L_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(B, L_k, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, L_v, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if self.use_rope:\n",
    "            B, H, L, D = Q.shape\n",
    "            Q = self.rope(Q.reshape(B * H, L, D)).reshape(B, H, L, D)\n",
    "            K = self.rope(K.reshape(B * H, L, D)).reshape(B, H ,L, D)\n",
    "\n",
    "        # [B, num_heads, L, L]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # [B, H, L_q, head_dim]\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # [B, L_q, D]\n",
    "        context = context.transpose(1, 2).contiguous().view(B, L_q, self.model_dim)\n",
    "        out = self.out_proj(context)\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_dim: int, \n",
    "        num_heads: int, \n",
    "        ff_hidden_dim=2048, \n",
    "        dropout=0.1,\n",
    "        use_rope: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(model_dim, num_heads, dropout, use_rope)\n",
    "        self.ffn = FeedForward(model_dim, ff_hidden_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # [B, L, D]\n",
    "        attn_out = self.mha(x, x, x, mask)\n",
    "        attn_out = self.norm1(x + self.dropout(attn_out))\n",
    "\n",
    "        # [B, L, D]\n",
    "        ffn_out = self.ffn(attn_out)\n",
    "        ffn_out = self.norm2(attn_out + self.dropout(ffn_out))\n",
    "\n",
    "        return ffn_out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_dim, \n",
    "        num_heads, \n",
    "        ff_hidden_dim=2048, \n",
    "        dropout=0.1,\n",
    "        use_rope: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(model_dim, num_heads, dropout, use_rope)\n",
    "        self.cross_attn = MultiHeadAttention(model_dim, num_heads, dropout, use_rope)\n",
    "        self.ffn = FeedForward(model_dim, ff_hidden_dim, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.norm3 = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, T, D]\n",
    "            enc_output: [B, L, D]\n",
    "            src_mask: [B, 1, 1, L]\n",
    "            tgt_mask: [B, 1, T, T]\n",
    "        Returns:\n",
    "            [B, T, D]\n",
    "        \"\"\"\n",
    "        self_attn_out = self.self_attn(x, x, x, tgt_mask)\n",
    "        x2 = self.norm1(x + self.dropout(self_attn_out))\n",
    "\n",
    "        cross_attn_out = self.cross_attn(x2, enc_output, enc_output, src_mask)\n",
    "        x3 = self.norm2(x2 + self.dropout(cross_attn_out))\n",
    "\n",
    "        ffn_out = self.ffn(x3)\n",
    "        x4 = self.norm3(x3 + self.dropout(ffn_out))\n",
    "\n",
    "        return x4\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        src_vocab_size, \n",
    "        tgt_vocab_size, \n",
    "        model_dim=512, \n",
    "        num_heads=8, \n",
    "        num_enc_layers=6, \n",
    "        num_dec_layers=6, \n",
    "        ff_hidden_dim=2048, \n",
    "        max_len_src=150,\n",
    "        max_len_trg=180,\n",
    "        dropout=0.1,\n",
    "        pos_type: Literal['pos', 'rope'] = 'pos'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.pos_type = pos_type\n",
    "        \n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, model_dim)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, model_dim)\n",
    "        if self.pos_type == 'pos':\n",
    "            self.encoder_pe = PositionalEncoding(model_dim, max_len_src)\n",
    "            self.decoder_pe = PositionalEncoding(model_dim, max_len_trg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            Encoder(model_dim, num_heads, ff_hidden_dim, dropout, use_rope=(pos_type == 'rope'))\n",
    "            for _ in range(num_enc_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            Decoder(model_dim, num_heads, ff_hidden_dim, dropout, use_rope=(pos_type=='rope'))\n",
    "            for _ in range(num_dec_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output_proj = nn.Linear(model_dim, tgt_vocab_size)\n",
    "    \n",
    "    def encode(self, src, src_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [B, S]\n",
    "            src_mask: [B, 1, 1, S]\n",
    "        Returns: [B, S, D]\n",
    "        \"\"\"\n",
    "        x = self.src_embedding(src) * math.sqrt(self.model_dim)\n",
    "        if self.pos_type == 'pos':\n",
    "            x = self.encoder_pe(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt, enc_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: [B, T]\n",
    "            enc_output: [B, L, D]\n",
    "            src_mask: [B, 1, 1, S]\n",
    "            tgt_mask: [B, 1, T, T]\n",
    "        Returns: [B, T, D]\n",
    "        \"\"\"\n",
    "        x = self.tgt_embedding(tgt) * math.sqrt(self.model_dim)\n",
    "        if self.pos_type == 'pos':\n",
    "            x = self.decoder_pe(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [B, L]\n",
    "            tgt: [B, T]\n",
    "        Returns: [B, T, tgt_vocab_size]\n",
    "        \"\"\"\n",
    "\n",
    "        # [B, S, D]\n",
    "        enc_output = self.encode(src, src_mask) \n",
    "\n",
    "        # [B, T, D]\n",
    "        dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        # [B, T, V]\n",
    "        logits = self.output_proj(dec_output) \n",
    "        return logits\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = html.unescape(text)\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_dataset(dataset: Dataset, ignore: Literal['vi', 'en', None] = None):\n",
    "    def _run(example):\n",
    "        example['raw_en'] = example['en']\n",
    "        example['raw_vi'] = example['vi']\n",
    "        if ignore != 'vi':\n",
    "            example['vi'] = preprocess_text(example['vi'])\n",
    "        if ignore != 'en':\n",
    "            example['en'] = preprocess_text(example['en'])\n",
    "        return example\n",
    "    \n",
    "    return dataset.map(_run)\n",
    "\n",
    "def invert_html_sign(text: str) -> str:\n",
    "    html_map = {\n",
    "        '&': '&amp;',\n",
    "        '<': '&lt;',\n",
    "        '>': '&gt;',\n",
    "        '\"': '&quot;',\n",
    "        \"'\": '&apos;',\n",
    "    }\n",
    "    for sign, value in html_map.items():\n",
    "        text = text.replace(sign, value)\n",
    "    return text\n",
    "\n",
    "def postprocess_text(raw_input: str, pred: str) -> str:\n",
    "    pred = pred.strip()\n",
    "    pred = invert_html_sign(pred)\n",
    "    entities = re.findall(r\"&[a-z]+;\", pred)\n",
    "    for i, ent in enumerate(entities):\n",
    "        pred = pred.replace(ent, f\"@@{i}@@\")\n",
    "    pred = re.sub(r\"([,.\\?!'\\\":;])\", r' \\1 ', pred)\n",
    "    for i, ent in enumerate(entities):\n",
    "        pred = pred.replace(f\"@@{i}@@\", ent)\n",
    "    \n",
    "    raw_input_words = raw_input.split()\n",
    "    start_id = 0\n",
    "    while start_id < len(raw_input_words):\n",
    "        if not re.fullmatch(r\"[A-Za-z√Ä-·ªπ0-9]+\", raw_input_words[start_id]):\n",
    "            start_id += 1\n",
    "        else:\n",
    "            break\n",
    "    upper_words = []\n",
    "    for i, word in enumerate(raw_input_words):\n",
    "        if i <= start_id:\n",
    "            continue\n",
    "        if word[0].isupper() and raw_input_words[i - 1][0] not in ['.', '?', '!']:\n",
    "            upper_words.append(word)\n",
    "    for word in upper_words:\n",
    "        pred = pred.replace(word.lower(), word)\n",
    "\n",
    "    pred_words = pred.split()\n",
    "    all_words = []\n",
    "    start_id = 0\n",
    "    while start_id < len(pred_words):\n",
    "        if not re.fullmatch(r\"[A-Za-z√Ä-·ªπ0-9]+\", pred_words[start_id]):\n",
    "            start_id += 1\n",
    "        else:\n",
    "            break\n",
    "    for i, word in enumerate(pred_words):\n",
    "        if i < start_id:\n",
    "            all_words.append(word)\n",
    "        elif i == start_id or pred_words[i - 1] in ['.', '?', '!']:\n",
    "            all_words.append(word[0].upper() + word[1:])\n",
    "        else:\n",
    "            all_words.append(word)\n",
    "    return ' '.join(all_words)\n",
    "\n",
    "UNK_TOKEN = '<unk>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "SOS_TOKEN = '<sos>' # Start of Sentence\n",
    "EOS_TOKEN = '<eos>' # End of Sentence\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2):\n",
    "        self.itos = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
    "        self.stoi = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4 \n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[UNK_TOKEN]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "\n",
    "class SubwordVocabulary:\n",
    "    def __init__(self, spm_model_path):\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(spm_model_path)\n",
    "\n",
    "        self.pad_idx = self.sp.pad_id()\n",
    "        self.sos_idx = self.sp.bos_id()\n",
    "        self.eos_idx = self.sp.eos_id()\n",
    "        self.unk_idx = self.sp.unk_id()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.sp.get_piece_size()\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        return self.sp.encode(text, out_type=int)\n",
    "\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self, dataset, src_vocab, trg_vocab, src_lang='en', trg_lang='vi'):\n",
    "        self.dataset = dataset\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.src_lang = src_lang\n",
    "        self.trg_lang = trg_lang\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raw_src_text = self.dataset[index][f'raw_{self.src_lang}']\n",
    "        src_text = self.dataset[index][self.src_lang]\n",
    "        raw_trg_text = self.dataset[index][f'raw_{self.trg_lang}']\n",
    "        trg_text = self.dataset[index][self.trg_lang]\n",
    "\n",
    "        src_numericalized = [self.src_vocab.stoi[SOS_TOKEN]]\n",
    "        src_numericalized += self.src_vocab.numericalize(src_text)\n",
    "        src_numericalized.append(self.src_vocab.stoi[EOS_TOKEN])\n",
    "\n",
    "        trg_numericalized = [self.trg_vocab.stoi[SOS_TOKEN]]\n",
    "        trg_numericalized += self.trg_vocab.numericalize(trg_text)\n",
    "        trg_numericalized.append(self.trg_vocab.stoi[EOS_TOKEN])\n",
    "\n",
    "        return raw_src_text, torch.tensor(src_numericalized), raw_trg_text, torch.tensor(trg_numericalized)\n",
    "\n",
    "class SpmBilingualDataset(Dataset):\n",
    "    def __init__(self, dataset, src_vocab, trg_vocab, src_lang='en', trg_lang='vi'):\n",
    "        self.dataset = dataset\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.src_lang = src_lang\n",
    "        self.trg_lang = trg_lang\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        raw_src_text = self.dataset[idx][f'raw_{self.src_lang}']\n",
    "        src_ids = (\n",
    "            [self.src_vocab.sos_idx]\n",
    "            + self.src_vocab.numericalize(self.dataset[idx][self.src_lang])\n",
    "            + [self.src_vocab.eos_idx]\n",
    "        )\n",
    "\n",
    "        raw_trg_text = self.dataset[idx][f'raw_{self.trg_lang}']\n",
    "        trg_ids = (\n",
    "            [self.trg_vocab.sos_idx]\n",
    "            + self.trg_vocab.numericalize(self.dataset[idx][self.trg_lang])\n",
    "            + [self.trg_vocab.eos_idx]\n",
    "        )\n",
    "\n",
    "        return raw_src_text, torch.tensor(src_ids), raw_trg_text, torch.tensor(trg_ids)\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, pad_idx, max_src_len=None, max_trg_len=None):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_trg_len = max_trg_len\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        raw_src = [item[0] for item in batch]\n",
    "        src = [item[1] for item in batch]\n",
    "        raw_trg = [item[2] for item in batch]\n",
    "        trg = [item[3] for item in batch]\n",
    "        if self.max_src_len is not None:\n",
    "            src = [s[:self.max_src_len] for s in src]\n",
    "        if self.max_trg_len is not None:\n",
    "            trg = [t[:self.max_trg_len] for t in trg]\n",
    "\n",
    "        src = pad_sequence(src, batch_first=True, padding_value=self.pad_idx)\n",
    "        trg = pad_sequence(trg, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        return raw_src, src, raw_trg, trg\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Evaluator ƒë·ªÉ ƒë√°nh gi√° model translation quality v·ªõi BLEU v√† ROUGE-L scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, test_loader, src_vocab, tgt_vocab, device, use_subword: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Trained Transformer model\n",
    "            test_loader: DataLoader cho test set\n",
    "            src_vocab: Source vocabulary\n",
    "            tgt_vocab: Target vocabulary\n",
    "            device: torch.device\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.test_loader = test_loader\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.device = device\n",
    "        self.use_subword = use_subword\n",
    "        \n",
    "        self.bleu_metric = BLEU(tokenize='none' if use_subword else '13a')\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "    \n",
    "    def indices_to_sentence(self, indices, vocab, remove_special=True):\n",
    "        \"\"\"\n",
    "        Convert token indices sang sentence string.\n",
    "        \n",
    "        Args:\n",
    "            indices: list or tensor - Token indices\n",
    "            vocab: Vocabulary object\n",
    "            remove_special: bool - Remove special tokens (<sos>, <eos>, <pad>)\n",
    "            \n",
    "        Returns:\n",
    "            sentence: str\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(indices):\n",
    "            indices = indices.tolist()\n",
    "        if self.use_subword:\n",
    "            return vocab.sp.decode_ids(indices)\n",
    "        \n",
    "        words = [vocab.itos[idx] for idx in indices]\n",
    "        words = [w for w in words if w not in ['<sos>', '<eos>', '<pad>']]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def evaluate_with_decoder(self, decoder, desc=\"Evaluation\"):\n",
    "        \"\"\"\n",
    "        Evaluate model s·ª≠ d·ª•ng decoder c·ª• th·ªÉ (Greedy ho·∫∑c Beam Search).\n",
    "        \n",
    "        Args:\n",
    "            decoder: GreedySearchDecoder ho·∫∑c BeamSearchDecoder instance\n",
    "            desc: str - Description cho progress bar\n",
    "            \n",
    "        Returns:\n",
    "            results: dict - Contains BLEU, ROUGE-L scores v√† examples\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        references = []  # Ground truth translations\n",
    "        hypotheses = []  # Model predictions\n",
    "        rouge_scores = []\n",
    "        \n",
    "        # Sample translations ƒë·ªÉ show\n",
    "        examples = []\n",
    "        num_examples = 5\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä {desc}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (raw_src_batch, src_batch, raw_tgt_batch, tgt_batch) in enumerate(tqdm(self.test_loader, desc=desc)):\n",
    "                batch_size = src_batch.size(0)\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    raw_src = raw_src_batch[i]\n",
    "                    src = src_batch[i:i+1]  # [1, S]\n",
    "                    raw_tgt = raw_tgt_batch[i]\n",
    "                    tgt = tgt_batch[i]  # [T]\n",
    "                    \n",
    "                    # Decode\n",
    "                    pred_indices = decoder.decode(src, self.src_vocab, self.tgt_vocab, self.device)\n",
    "                    \n",
    "                    # Convert to sentences\n",
    "                    pred_sentence = self.indices_to_sentence(pred_indices, self.tgt_vocab)\n",
    "                    pred_sentence = postprocess_text(raw_input=raw_src, pred=pred_sentence)\n",
    "                    # ref_sentence = self.indices_to_sentence(tgt, self.tgt_vocab)\n",
    "                    ref_sentence = raw_tgt\n",
    "                    src_sentence = self.indices_to_sentence(src[0], self.src_vocab)\n",
    "                    \n",
    "                    # Collect for metrics\n",
    "                    hypotheses.append(pred_sentence)\n",
    "                    references.append(ref_sentence)\n",
    "                    \n",
    "                    # Calculate ROUGE-L for this pair\n",
    "                    rouge_result = self.rouge_scorer.score(ref_sentence, pred_sentence)\n",
    "                    rouge_scores.append(rouge_result['rougeL'].fmeasure)\n",
    "                    \n",
    "                    # Save examples\n",
    "                    if len(examples) < num_examples:\n",
    "                        examples.append({\n",
    "                            'source': src_sentence,\n",
    "                            'reference': ref_sentence,\n",
    "                            'prediction': pred_sentence\n",
    "                        })\n",
    "        \n",
    "        # Calculate BLEU\n",
    "        # sacrebleu expects list of references for each hypothesis\n",
    "        bleu_score = self.bleu_metric.corpus_score(hypotheses, [references])\n",
    "        \n",
    "        # Average ROUGE-L\n",
    "        avg_rouge = np.mean(rouge_scores)\n",
    "        \n",
    "        results = {\n",
    "            'bleu': bleu_score.score,\n",
    "            'rouge_l': avg_rouge,\n",
    "            'num_samples': len(hypotheses),\n",
    "            'examples': examples\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìà Results:\")\n",
    "        print(f\"   BLEU Score:   {bleu_score.score:.2f}\")\n",
    "        print(f\"   ROUGE-L F1:   {avg_rouge:.4f}\")\n",
    "        print(f\"   Samples:      {len(hypotheses)}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Print examples\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"üìù Translation Examples:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for idx, ex in enumerate(examples, 1):\n",
    "            print(f\"\\nExample {idx}:\")\n",
    "            print(f\"  Source:     {ex['source']}\")\n",
    "            print(f\"  Reference:  {ex['reference']}\")\n",
    "            print(f\"  Prediction: {ex['prediction']}\")\n",
    "        print(f\"\\n{'='*60}\\n\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compare_decoders(self, greedy_decoder, beam_decoder):\n",
    "        \"\"\"\n",
    "        So s√°nh Greedy Search vs Beam Search.\n",
    "        \n",
    "        Args:\n",
    "            greedy_decoder: GreedySearchDecoder instance\n",
    "            beam_decoder: BeamSearchDecoder instance\n",
    "            \n",
    "        Returns:\n",
    "            comparison: dict - Results t·ª´ c·∫£ 2 decoders\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üîç COMPARING DECODING STRATEGIES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        greedy_results = self.evaluate_with_decoder(greedy_decoder, \"Greedy Search\")\n",
    "        beam_results = self.evaluate_with_decoder(beam_decoder, f\"Beam Search (k={beam_decoder.beam_size})\")\n",
    "        \n",
    "        # Summary comparison\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä COMPARISON SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\n{'Method':<20} {'BLEU':<10} {'ROUGE-L':<10}\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"{'Greedy Search':<20} {greedy_results['bleu']:<10.2f} {greedy_results['rouge_l']:<10.4f}\")\n",
    "        print(f\"{'Beam Search':<20} {beam_results['bleu']:<10.2f} {beam_results['rouge_l']:<10.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        improvement_bleu = beam_results['bleu'] - greedy_results['bleu']\n",
    "        improvement_rouge = beam_results['rouge_l'] - greedy_results['rouge_l']\n",
    "        \n",
    "        print(f\"{'Improvement':<20} {improvement_bleu:<10.2f} {improvement_rouge:<10.4f}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return {\n",
    "            'greedy': greedy_results,\n",
    "            'beam': beam_results,\n",
    "            'improvement': {\n",
    "                'bleu': improvement_bleu,\n",
    "                'rouge_l': improvement_rouge\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def calculate_bleu_score(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Helper function ƒë·ªÉ t√≠nh BLEU score.\n",
    "    \n",
    "    Args:\n",
    "        references: list of str - Ground truth translations\n",
    "        hypotheses: list of str - Model predictions\n",
    "        \n",
    "    Returns:\n",
    "        bleu_score: float\n",
    "    \"\"\"\n",
    "    bleu = BLEU()\n",
    "    score = bleu.corpus_score(hypotheses, [references])\n",
    "    return score.score\n",
    "\n",
    "\n",
    "def calculate_rouge_score(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Helper function ƒë·ªÉ t√≠nh ROUGE-L score.\n",
    "    \n",
    "    Args:\n",
    "        references: list of str - Ground truth translations\n",
    "        hypotheses: list of str - Model predictions\n",
    "        \n",
    "    Returns:\n",
    "        avg_rouge_l: float - Average ROUGE-L F1 score\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "    scores = []\n",
    "    \n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        result = scorer.score(ref, hyp)\n",
    "        scores.append(result['rougeL'].fmeasure)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "class GreedySearchDecoder:\n",
    "    \"\"\"\n",
    "    Greedy Search: Ch·ªçn token c√≥ x√°c su·∫•t cao nh·∫•t ·ªü m·ªói b∆∞·ªõc.\n",
    "    Nhanh nh∆∞ng quality th·∫•p h∆°n Beam Search.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, max_len=100, use_subword=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Trained Transformer model\n",
    "            max_len: Maximum length c·ªßa generated sequence\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.max_len = max_len\n",
    "        self.use_subword = use_subword\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def decode(self, src, src_vocab, tgt_vocab, device):\n",
    "        \"\"\"\n",
    "        Greedy decode m·ªôt c√¢u source.\n",
    "        \n",
    "        Args:\n",
    "            src: [1, S] - Source tensor (batch_size=1)\n",
    "            src_vocab: Vocabulary object cho source\n",
    "            tgt_vocab: Vocabulary object cho target\n",
    "            device: torch.device\n",
    "            \n",
    "        Returns:\n",
    "            decoded_tokens: list of int - Token IDs\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        src = src.to(device)\n",
    "        if self.use_subword:\n",
    "            pad_idx = src_vocab.pad_idx\n",
    "            sos_idx = src_vocab.sos_idx\n",
    "            eos_idx = src_vocab.eos_idx\n",
    "        else:\n",
    "            pad_idx = src_vocab.stoi['<pad>']\n",
    "            sos_idx = tgt_vocab.stoi['<sos>']\n",
    "            eos_idx = tgt_vocab.stoi['<eos>']\n",
    "        \n",
    "        # Encode source\n",
    "        src_mask = create_padding_mask(src, pad_idx).to(device)\n",
    "        enc_output = self.model.encode(src, src_mask)  # [1, S, D]\n",
    "        \n",
    "        # Start v·ªõi <sos> token\n",
    "        decoded_tokens = [sos_idx]\n",
    "        \n",
    "        for _ in range(self.max_len):\n",
    "            # T·∫°o target tensor t·ª´ tokens ƒë√£ decode\n",
    "            tgt = torch.LongTensor([decoded_tokens]).to(device)  # [1, T]\n",
    "            \n",
    "            # Create target mask\n",
    "            tgt_mask = create_causal_mask(len(decoded_tokens), device)\n",
    "            \n",
    "            # Decode\n",
    "            dec_output = self.model.decode(tgt, enc_output, src_mask, tgt_mask)  # [1, T, D]\n",
    "            \n",
    "            # Get logits cho token cu·ªëi c√πng\n",
    "            logits = self.model.output_proj(dec_output[:, -1, :])  # [1, V]\n",
    "            \n",
    "            # Greedy: ch·ªçn token c√≥ prob cao nh·∫•t\n",
    "            next_token = logits.argmax(dim=-1).item()\n",
    "            \n",
    "            decoded_tokens.append(next_token)\n",
    "            \n",
    "            # Stop n·∫øu g·∫∑p <eos>\n",
    "            if next_token == eos_idx:\n",
    "                break\n",
    "        \n",
    "        return decoded_tokens\n",
    "    \n",
    "    def translate(self, src_sentence, src_vocab, tgt_vocab, device):\n",
    "        \"\"\"\n",
    "        Translate m·ªôt c√¢u t·ª´ source sang target language.\n",
    "        \n",
    "        Args:\n",
    "            src_sentence: str - C√¢u source (ƒë√£ tokenized)\n",
    "            src_vocab: Vocabulary cho source\n",
    "            tgt_vocab: Vocabulary cho target\n",
    "            device: torch.device\n",
    "            \n",
    "        Returns:\n",
    "            translation: str - C√¢u ƒë√£ d·ªãch\n",
    "        \"\"\"\n",
    "        # Tokenize v√† convert sang tensor\n",
    "        if self.use_subword:\n",
    "            src_ids = (\n",
    "                [src_vocab.sos_idx]\n",
    "                + src_vocab.encode(src_sentence)\n",
    "                + [src_vocab.eos_idx]\n",
    "            )\n",
    "        else:\n",
    "            tokens = src_sentence.split()\n",
    "            src_ids = [\n",
    "                src_vocab.stoi.get(tok, src_vocab.stoi['<unk>'])\n",
    "                for tok in tokens\n",
    "            ]\n",
    "        src_tensor = torch.LongTensor([src_ids]).to(device)  # [1, S]\n",
    "        \n",
    "        # Decode\n",
    "        decoded_ids = self.decode(src_tensor, src_vocab, tgt_vocab, device)\n",
    "        \n",
    "        if self.use_subword:\n",
    "            return tgt_vocab.decode(decoded_ids)\n",
    "        else:\n",
    "            words = [\n",
    "                tgt_vocab.itos[i]\n",
    "                for i in decoded_ids\n",
    "                if tgt_vocab.itos[i] not in ['<sos>', '<eos>', '<pad>']\n",
    "            ]\n",
    "            return ' '.join(words)\n",
    "\n",
    "\n",
    "class BeamSearchDecoder:\n",
    "    \"\"\"\n",
    "    Beam Search: Maintain top-k hypotheses ƒë·ªÉ t√¨m translation t·ªët h∆°n.\n",
    "    Ch·∫≠m h∆°n nh∆∞ng quality cao h∆°n Greedy Search.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, beam_size=5, max_len=100, length_penalty=0.6, use_subword=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Trained Transformer model\n",
    "            beam_size: S·ªë l∆∞·ª£ng beams (hypotheses) ƒë·ªÉ maintain\n",
    "            max_len: Maximum length c·ªßa generated sequence\n",
    "            length_penalty: Alpha parameter cho length normalization\n",
    "                           (0.0 = no penalty, 1.0 = full penalty)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.beam_size = beam_size\n",
    "        self.max_len = max_len\n",
    "        self.length_penalty = length_penalty\n",
    "        self.use_subword = use_subword\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def decode(self, src, src_vocab, tgt_vocab, device):\n",
    "        \"\"\"\n",
    "        Beam search decode.\n",
    "        \n",
    "        Args:\n",
    "            src: [1, S] - Source tensor\n",
    "            src_vocab: Vocabulary cho source\n",
    "            tgt_vocab: Vocabulary cho target\n",
    "            device: torch.device\n",
    "            \n",
    "        Returns:\n",
    "            best_sequence: list of int - Best decoded token IDs\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        src = src.to(device)\n",
    "        if self.use_subword:\n",
    "            pad_idx = src_vocab.pad_idx\n",
    "            sos_idx = tgt_vocab.sos_idx\n",
    "            eos_idx = tgt_vocab.eos_idx\n",
    "        else:\n",
    "            pad_idx = src_vocab.stoi['<pad>']\n",
    "            sos_idx = tgt_vocab.stoi['<sos>']\n",
    "            eos_idx = tgt_vocab.stoi['<eos>']\n",
    "        \n",
    "        # Encode source\n",
    "        src_mask = create_padding_mask(src, pad_idx).to(device)\n",
    "        enc_output = self.model.encode(src, src_mask)  # [1, S, D]\n",
    "        \n",
    "        # Initialize beams\n",
    "        # Each beam: (score, tokens)\n",
    "        beams = [(0.0, [sos_idx])]\n",
    "        completed_beams = []\n",
    "        \n",
    "        for step in range(self.max_len):\n",
    "            candidates = []\n",
    "            \n",
    "            for score, tokens in beams:\n",
    "                # N·∫øu beam ƒë√£ k·∫øt th√∫c, add v√†o completed\n",
    "                if tokens[-1] == eos_idx:\n",
    "                    completed_beams.append((score, tokens))\n",
    "                    continue\n",
    "                \n",
    "                # T·∫°o target tensor\n",
    "                tgt = torch.LongTensor([tokens]).to(device)  # [1, T]\n",
    "                tgt_mask = create_causal_mask(len(tokens), device)\n",
    "                \n",
    "                # Decode\n",
    "                dec_output = self.model.decode(tgt, enc_output, src_mask, tgt_mask)\n",
    "                logits = self.model.output_proj(dec_output[:, -1, :])  # [1, V]\n",
    "                \n",
    "                # Get log probabilities\n",
    "                log_probs = F.log_softmax(logits, dim=-1)  # [1, V]\n",
    "                \n",
    "                # Get top-k tokens\n",
    "                topk_log_probs, topk_indices = log_probs.topk(self.beam_size, dim=-1)\n",
    "                \n",
    "                # Create new candidates\n",
    "                for i in range(self.beam_size):\n",
    "                    token_id = topk_indices[0, i].item()\n",
    "                    token_score = topk_log_probs[0, i].item()\n",
    "                    \n",
    "                    new_score = score + token_score\n",
    "                    new_tokens = tokens + [token_id]\n",
    "                    \n",
    "                    candidates.append((new_score, new_tokens))\n",
    "            \n",
    "            # Kh√¥ng c√≤n candidates n√†o\n",
    "            if not candidates:\n",
    "                break\n",
    "            \n",
    "            # Ch·ªçn top-k beams theo score\n",
    "            # Apply length normalization: score / (len ** alpha)\n",
    "            candidates_normalized = [\n",
    "                (score / (len(tokens) ** self.length_penalty), score, tokens)\n",
    "                for score, tokens in candidates\n",
    "            ]\n",
    "            candidates_normalized.sort(reverse=True, key=lambda x: x[0])\n",
    "            \n",
    "            # Keep top beam_size\n",
    "            beams = [(score, tokens) for _, score, tokens in candidates_normalized[:self.beam_size]]\n",
    "            \n",
    "            # Early stopping: n·∫øu ƒë√£ c√≥ ƒë·ªß completed beams\n",
    "            if len(completed_beams) >= self.beam_size:\n",
    "                break\n",
    "        \n",
    "        # Add remaining beams to completed\n",
    "        completed_beams.extend(beams)\n",
    "        \n",
    "        # Ch·ªçn best beam (normalize by length)\n",
    "        if completed_beams:\n",
    "            best = max(completed_beams, key=lambda x: x[0] / (len(x[1]) ** self.length_penalty))\n",
    "            return best[1]\n",
    "        else:\n",
    "            # Fallback: return beam ƒë·∫ßu ti√™n\n",
    "            return beams[0][1] if beams else [sos_idx, eos_idx]\n",
    "    \n",
    "    def translate(self, src_sentence, src_vocab, tgt_vocab, device):\n",
    "        \"\"\"\n",
    "        Translate m·ªôt c√¢u s·ª≠ d·ª•ng beam search.\n",
    "        \n",
    "        Args:\n",
    "            src_sentence: str - C√¢u source\n",
    "            src_vocab: Vocabulary cho source\n",
    "            tgt_vocab: Vocabulary cho target\n",
    "            device: torch.device\n",
    "            \n",
    "        Returns:\n",
    "            translation: str - C√¢u ƒë√£ d·ªãch\n",
    "        \"\"\"\n",
    "        # Tokenize\n",
    "        if self.use_subword:\n",
    "            src_ids = (\n",
    "                [src_vocab.sos_idx]\n",
    "                + src_vocab.encode(src_sentence)\n",
    "                + [src_vocab.eos_idx]\n",
    "            )\n",
    "        else:\n",
    "            tokens = src_sentence.split()\n",
    "            src_ids = [\n",
    "                src_vocab.stoi.get(tok, src_vocab.stoi['<unk>'])\n",
    "                for tok in tokens\n",
    "            ]\n",
    "        src_tensor = torch.LongTensor([src_ids]).to(device)\n",
    "        \n",
    "        # Decode\n",
    "        decoded_ids = self.decode(src_tensor, src_vocab, tgt_vocab, device)\n",
    "        \n",
    "        if self.use_subword:\n",
    "            return tgt_vocab.decode(decoded_ids)\n",
    "        else:\n",
    "            words = [\n",
    "                tgt_vocab.itos[i]\n",
    "                for i in decoded_ids\n",
    "                if tgt_vocab.itos[i] not in ['<sos>', '<eos>', '<pad>']\n",
    "            ]\n",
    "            return ' '.join(words)\n",
    "\n",
    "def create_padding_mask(seq, pad_idx):\n",
    "    \"\"\"\n",
    "    T·∫°o padding mask cho attention.\n",
    "\n",
    "    Args:\n",
    "        seq: [B, L] - Input sequence (token ids)\n",
    "        pad_idx: int - Index c·ªßa padding token\n",
    "\n",
    "    Returns:\n",
    "        mask: [B, 1, 1, L]\n",
    "              Gi√° tr·ªã:\n",
    "                - 1.0 : token h·ª£p l·ªá (ƒë∆∞·ª£c attend)\n",
    "                - 0.0 : padding token (b·ªã mask / ignore)\n",
    "    \"\"\"\n",
    "    # [B, L] -> [B, 1, 1, L]\n",
    "    mask = (seq != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    return mask.float()\n",
    "\n",
    "\n",
    "def create_causal_mask(seq_len, device='cpu'):\n",
    "    \"\"\"\n",
    "    T·∫°o causal (look-ahead) mask cho decoder self-attention.\n",
    "\n",
    "    Args:\n",
    "        seq_len: int - ƒê·ªô d√†i sequence\n",
    "        device: torch.device ho·∫∑c str\n",
    "\n",
    "    Returns:\n",
    "        mask: [1, 1, seq_len, seq_len]\n",
    "              Gi√° tr·ªã:\n",
    "                - 1.0 : ƒë∆∞·ª£c attend\n",
    "                - 0.0 : b·ªã ch·∫∑n (future tokens)\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).bool()\n",
    "    return mask.unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "\n",
    "def create_masks(src, tgt, pad_idx, device='cpu'):\n",
    "    \"\"\"\n",
    "    T·∫°o c√°c mask c·∫ßn thi·∫øt cho Transformer encoder-decoder.\n",
    "\n",
    "    Args:\n",
    "        src: [B, S] - Source token ids\n",
    "        tgt: [B, T] - Target token ids\n",
    "        pad_idx: int - Padding token index\n",
    "        device: torch.device ho·∫∑c str\n",
    "\n",
    "    Returns:\n",
    "        src_mask: [B, 1, 1, S]\n",
    "                  (1 = attend, 0 = padding)\n",
    "\n",
    "        tgt_mask: [B, 1, T, T]\n",
    "                  K·∫øt h·ª£p:\n",
    "                    - padding mask\n",
    "                    - causal mask\n",
    "                  (1 = attend, 0 = masked)\n",
    "    \"\"\"\n",
    "    # Source padding mask\n",
    "    src_mask = create_padding_mask(src, pad_idx)\n",
    "    \n",
    "    # Target padding mask\n",
    "    tgt_padding_mask = create_padding_mask(tgt, pad_idx)  # [B, 1, 1, T]\n",
    "    \n",
    "    # Target causal mask\n",
    "    tgt_len = tgt.size(1)\n",
    "    tgt_causal_mask = create_causal_mask(tgt_len, device)  # [1, 1, T, T]\n",
    "    \n",
    "    # Combine: padding mask OR causal mask\n",
    "    tgt_padding_mask = tgt_padding_mask.expand(-1, -1, tgt_len, -1)\n",
    "    tgt_mask = tgt_padding_mask * tgt_causal_mask  # [B, 1, T, T]\n",
    "    \n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, filepath):\n",
    "    \"\"\"\n",
    "    L∆∞u model checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        model: nn.Module - Model c·∫ßn l∆∞u\n",
    "        optimizer: Optimizer\n",
    "        epoch: int - Current epoch\n",
    "        train_loss: float - Training loss\n",
    "        val_loss: float - Validation loss\n",
    "        filepath: str - ƒê∆∞·ªùng d·∫´n l∆∞u file\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"‚úÖ Checkpoint saved: {filepath}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    Load model checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        filepath: str - ƒê∆∞·ªùng d·∫´n file checkpoint\n",
    "        model: nn.Module - Model ƒë·ªÉ load weights\n",
    "        optimizer: Optimizer (optional) - Optimizer ƒë·ªÉ load state\n",
    "        device: str - Device\n",
    "        \n",
    "    Returns:\n",
    "        epoch: int - Epoch ƒë√£ train\n",
    "        train_loss: float\n",
    "        val_loss: float\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    print(f\"‚úÖ Checkpoint loaded: {filepath}\")\n",
    "    print(f\"   Epoch: {checkpoint['epoch']}, Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    return checkpoint['epoch'], checkpoint['train_loss'], checkpoint['val_loss']\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    ƒê·∫øm s·ªë l∆∞·ª£ng parameters c·ªßa model.\n",
    "    \n",
    "    Args:\n",
    "        model: nn.Module\n",
    "        \n",
    "    Returns:\n",
    "        total: int - T·ªïng s·ªë parameters\n",
    "        trainable: int - S·ªë parameters c√≥ th·ªÉ train\n",
    "    \"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device,\n",
    "        pad_idx,\n",
    "        checkpoint_dir='checkpoints',\n",
    "        log_dir='logs'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trainer class cho Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            model: Transformer model\n",
    "            train_loader: DataLoader cho training\n",
    "            val_loader: DataLoader cho validation\n",
    "            optimizer: Optimizer (Adam)\n",
    "            criterion: Loss function (CrossEntropyLoss)\n",
    "            device: torch.device\n",
    "            pad_idx: Padding token index\n",
    "            checkpoint_dir: Th∆∞ m·ª•c l∆∞u checkpoints\n",
    "            log_dir: Th∆∞ m·ª•c l∆∞u logs\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # Create directories\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        self.log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        self.best_val_loss = float('inf')\n",
    "        \n",
    "    def train_epoch(self, epoch, warmup_scheduler=None):\n",
    "        \"\"\"\n",
    "        Train m·ªôt epoch.\n",
    "        \n",
    "        Args:\n",
    "            warmup_scheduler: WarmupScheduler - Warmup scheduler (optional)\n",
    "        \n",
    "        Returns:\n",
    "            avg_loss: float - Average training loss\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch} [Train]')\n",
    "        for batch_idx, (raw_src, src, raw_tgt, tgt) in enumerate(pbar):\n",
    "            src = src.to(self.device)  # [B, S]\n",
    "            tgt = tgt.to(self.device)  # [B, T]\n",
    "            \n",
    "            # Decoder input: b·ªè token cu·ªëi (<eos>)\n",
    "            tgt_input = tgt[:, :-1]  # [B, T-1]\n",
    "            \n",
    "            # Target output: b·ªè token ƒë·∫ßu (<sos>)\n",
    "            tgt_output = tgt[:, 1:]  # [B, T-1]\n",
    "            \n",
    "            # Create masks\n",
    "            src_mask, tgt_mask = create_masks(src, tgt_input, self.pad_idx, self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            logits = self.model(src, tgt_input, src_mask, tgt_mask)  # [B, T-1, V]\n",
    "            \n",
    "            # Calculate loss\n",
    "            # Reshape: [B, T-1, V] -> [B * T-1, V]\n",
    "            #          [B, T-1] -> [B * T-1]\n",
    "            logits = logits.reshape(-1, logits.size(-1))\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            \n",
    "            loss = self.criterion(logits, tgt_output)\n",
    "            \n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"\\n‚ö†Ô∏è Warning: NaN/Inf loss detected! Skipping batch.\")\n",
    "                continue\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping ƒë·ªÉ tr√°nh exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update warmup scheduler (per batch)\n",
    "            if warmup_scheduler is not None:\n",
    "                warmup_scheduler.step()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar with current LR\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{current_lr:.2e}'})\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        return avg_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, epoch):\n",
    "        \"\"\"\n",
    "        Validate model.\n",
    "        \n",
    "        Returns:\n",
    "            avg_loss: float - Average validation loss\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        pbar = tqdm(self.val_loader, desc=f'Epoch {epoch} [Val]  ')\n",
    "        for raw_src, src, raw_tgt, tgt in pbar:\n",
    "            src = src.to(self.device)\n",
    "            tgt = tgt.to(self.device)\n",
    "            \n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            src_mask, tgt_mask = create_masks(src, tgt_input, self.pad_idx, self.device)\n",
    "            \n",
    "            logits = self.model(src, tgt_input, src_mask, tgt_mask)\n",
    "            \n",
    "            logits = logits.reshape(-1, logits.size(-1))\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            \n",
    "            loss = self.criterion(logits, tgt_output)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        return avg_loss\n",
    "    \n",
    "    def train(self, num_epochs, warmup_scheduler=None, plateau_scheduler=None, patience=5):\n",
    "        \"\"\"\n",
    "        Training loop ch√≠nh.\n",
    "        \n",
    "        Args:\n",
    "            num_epochs: int - S·ªë epochs\n",
    "            warmup_scheduler: WarmupScheduler - Warmup scheduler (optional)\n",
    "            plateau_scheduler: ReduceLROnPlateau - Plateau scheduler (optional)\n",
    "            patience: int - Early stopping patience\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üöÄ B·∫ÆT ƒê·∫¶U TRAINING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total, trainable = count_parameters(self.model)\n",
    "        print(f\"üìä Model Parameters:\")\n",
    "        print(f\"   Total: {total:,}\")\n",
    "        print(f\"   Trainable: {trainable:,}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        epochs_no_improve = 0\n",
    "        \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            # Train\n",
    "            train_loss = self.train_epoch(epoch, warmup_scheduler)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss = self.validate(epoch)\n",
    "            \n",
    "            # Learning rate\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\nüìà Epoch {epoch}/{num_epochs} Summary:\")\n",
    "            print(f\"   Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"   Val Loss:   {val_loss:.4f}\")\n",
    "            print(f\"   LR:         {current_lr:.6f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "                \n",
    "                checkpoint_path = self.checkpoint_dir / 'best_model.pt'\n",
    "                save_checkpoint(\n",
    "                    self.model, \n",
    "                    self.optimizer, \n",
    "                    epoch, \n",
    "                    train_loss, \n",
    "                    val_loss, \n",
    "                    checkpoint_path\n",
    "                )\n",
    "                print(f\"   ‚ú® New best model! (Val Loss: {val_loss:.4f})\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                print(f\"   ‚è≥ No improvement for {epochs_no_improve} epoch(s)\")\n",
    "            \n",
    "            # Learning rate scheduling (plateau scheduler after warmup)\n",
    "            if plateau_scheduler is not None:\n",
    "                plateau_scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"\\n‚ö†Ô∏è  Early stopping triggered! No improvement for {patience} epochs.\")\n",
    "                break\n",
    "            \n",
    "            # Save checkpoint m·ªói 5 epochs\n",
    "            if epoch % 5 == 0:\n",
    "                checkpoint_path = self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pt'\n",
    "                save_checkpoint(\n",
    "                    self.model,\n",
    "                    self.optimizer,\n",
    "                    epoch,\n",
    "                    train_loss,\n",
    "                    val_loss,\n",
    "                    checkpoint_path\n",
    "                )\n",
    "            \n",
    "            print(\"-\" * 60 + \"\\n\")\n",
    "        \n",
    "        # Save training history\n",
    "        self.save_history()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "        print(f\"üìä Best Validation Loss: {self.best_val_loss:.4f}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    def save_history(self):\n",
    "        \"\"\"L∆∞u training history v√†o JSON file.\"\"\"\n",
    "        history_path = self.log_dir / 'training_history.json'\n",
    "        with open(history_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "        print(f\"üìù Training history saved: {history_path}\")\n",
    "\n",
    "\n",
    "def create_optimizer(model, learning_rate=1e-4, betas=(0.9, 0.98), eps=1e-9, weight_decay=1e-4):\n",
    "    \"\"\"\n",
    "    T·∫°o Adam optimizer v·ªõi hyperparameters chu·∫©n cho Transformer.\n",
    "    \n",
    "    Args:\n",
    "        model: nn.Module\n",
    "        learning_rate: float\n",
    "        betas: tuple - Adam beta parameters\n",
    "        eps: float - Epsilon for numerical stability\n",
    "        weight_decay: float - L2 regularization\n",
    "        \n",
    "    Returns:\n",
    "        optimizer: torch.optim.AdamW\n",
    "    \"\"\"\n",
    "    return torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=betas,\n",
    "        eps=eps,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "\n",
    "def create_scheduler(optimizer, mode='plateau', factor=0.5, patience=3, min_lr=1e-6):\n",
    "    \"\"\"\n",
    "    T·∫°o learning rate scheduler.\n",
    "    \n",
    "    Args:\n",
    "        optimizer: Optimizer\n",
    "        mode: str - 'plateau' ho·∫∑c 'step'\n",
    "        factor: float - Factor gi·∫£m learning rate\n",
    "        patience: int - S·ªë epochs ch·ªù tr∆∞·ªõc khi gi·∫£m LR\n",
    "        min_lr: float - Minimum learning rate\n",
    "        \n",
    "    Returns:\n",
    "        scheduler: Learning rate scheduler\n",
    "    \"\"\"\n",
    "    if mode == 'plateau':\n",
    "        return torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=factor,\n",
    "            patience=patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "    elif mode == 'step':\n",
    "        return torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=5,\n",
    "            gamma=factor\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler mode: {mode}\")\n",
    "\n",
    "\n",
    "class WarmupScheduler:\n",
    "    \"\"\"\n",
    "    Learning rate scheduler with warmup.\n",
    "    Implements the schedule from \"Attention is All You Need\" paper.\n",
    "    \n",
    "    lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer: Optimizer\n",
    "            d_model: int - Model dimension\n",
    "            warmup_steps: int - Number of warmup steps\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "        self._update_lr()\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Update learning rate.\"\"\"\n",
    "        self.current_step += 1\n",
    "        self._update_lr()\n",
    "    \n",
    "    def _update_lr(self):\n",
    "        \"\"\"Calculate and update learning rate.\"\"\"\n",
    "        step = max(self.current_step, 1)  # Avoid division by zero\n",
    "        lr = (self.d_model ** -0.5) * min(step ** -0.5, step * self.warmup_steps ** -1.5)\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    def get_last_lr(self):\n",
    "        \"\"\"Get current learning rate.\"\"\"\n",
    "        return [param_group['lr'] for param_group in self.optimizer.param_groups]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "\n",
    "def plot_training_curves(history_path, save_dir='figures'):\n",
    "    \"\"\"\n",
    "    V·∫Ω training v√† validation loss curves.\n",
    "    \n",
    "    Args:\n",
    "        history_path: str - Path to training_history.json\n",
    "        save_dir: str - Directory ƒë·ªÉ l∆∞u figures\n",
    "    \"\"\"\n",
    "    # Load history\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Plot loss curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-o', label='Train Loss', linewidth=2, markersize=4)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-s', label='Val Loss', linewidth=2, markersize=4)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate plot\n",
    "    ax2.plot(epochs, history['learning_rates'], 'g-^', linewidth=2, markersize=4)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Learning Rate', fontsize=12)\n",
    "    ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = save_dir / 'training_curves.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metrics_comparison(comparison_results, save_dir='figures'):\n",
    "    \"\"\"\n",
    "    V·∫Ω bi·ªÉu ƒë·ªì so s√°nh BLEU v√† ROUGE-L gi·ªØa Greedy v√† Beam Search.\n",
    "    \n",
    "    Args:\n",
    "        comparison_results: dict - Results t·ª´ Evaluator.compare_decoders()\n",
    "        save_dir: str - Directory ƒë·ªÉ l∆∞u figures\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Extract data\n",
    "    methods = ['Greedy Search', 'Beam Search']\n",
    "    bleu_scores = [\n",
    "        comparison_results['greedy']['bleu'],\n",
    "        comparison_results['beam']['bleu']\n",
    "    ]\n",
    "    rouge_scores = [\n",
    "        comparison_results['greedy']['rouge_l'],\n",
    "        comparison_results['beam']['rouge_l']\n",
    "    ]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # BLEU comparison\n",
    "    colors = ['#3498db', '#e74c3c']\n",
    "    bars1 = ax1.bar(methods, bleu_scores, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    ax1.set_ylabel('BLEU Score', fontsize=12)\n",
    "    ax1.set_title('BLEU Score Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylim(0, max(bleu_scores) * 1.2)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars1, bleu_scores):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{score:.2f}',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # ROUGE-L comparison\n",
    "    bars2 = ax2.bar(methods, rouge_scores, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    ax2.set_ylabel('ROUGE-L F1 Score', fontsize=12)\n",
    "    ax2.set_title('ROUGE-L Score Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylim(0, max(rouge_scores) * 1.2)\n",
    "    \n",
    "    for bar, score in zip(bars2, rouge_scores):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{score:.4f}',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = save_dir / 'metrics_comparison.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_histogram(history_path, save_dir='figures'):\n",
    "    \"\"\"\n",
    "    V·∫Ω histogram distribution c·ªßa train/val loss.\n",
    "    \n",
    "    Args:\n",
    "        history_path: str - Path to training_history.json\n",
    "        save_dir: str - Directory ƒë·ªÉ l∆∞u figures\n",
    "    \"\"\"\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax.hist(history['train_loss'], bins=20, alpha=0.5, label='Train Loss', color='blue', edgecolor='black')\n",
    "    ax.hist(history['val_loss'], bins=20, alpha=0.5, label='Val Loss', color='red', edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel('Loss Value', fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_title('Loss Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = save_dir / 'loss_histogram.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_summary_table(comparison_results, history_path, save_dir='figures'):\n",
    "    \"\"\"\n",
    "    T·∫°o summary table v·ªõi t·∫•t c·∫£ metrics.\n",
    "    \n",
    "    Args:\n",
    "        comparison_results: dict - Results t·ª´ comparison\n",
    "        history_path: str - Path to training history\n",
    "        save_dir: str - Directory ƒë·ªÉ l∆∞u\n",
    "    \"\"\"\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Prepare data\n",
    "    summary = {\n",
    "        'Training Summary': {\n",
    "            'Total Epochs': len(history['train_loss']),\n",
    "            'Final Train Loss': f\"{history['train_loss'][-1]:.4f}\",\n",
    "            'Final Val Loss': f\"{history['val_loss'][-1]:.4f}\",\n",
    "            'Best Val Loss': f\"{min(history['val_loss']):.4f}\",\n",
    "            'Final LR': f\"{history['learning_rates'][-1]:.6f}\"\n",
    "        },\n",
    "        'Greedy Search': {\n",
    "            'BLEU Score': f\"{comparison_results['greedy']['bleu']:.2f}\",\n",
    "            'ROUGE-L F1': f\"{comparison_results['greedy']['rouge_l']:.4f}\",\n",
    "            'Samples': comparison_results['greedy']['num_samples']\n",
    "        },\n",
    "        'Beam Search': {\n",
    "            'BLEU Score': f\"{comparison_results['beam']['bleu']:.2f}\",\n",
    "            'ROUGE-L F1': f\"{comparison_results['beam']['rouge_l']:.4f}\",\n",
    "            'Samples': comparison_results['beam']['num_samples']\n",
    "        },\n",
    "        'Improvement (Beam vs Greedy)': {\n",
    "            'BLEU': f\"{comparison_results['improvement']['bleu']:+.2f}\",\n",
    "            'ROUGE-L': f\"{comparison_results['improvement']['rouge_l']:+.4f}\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Prepare table data\n",
    "    table_data = []\n",
    "    for section, metrics in summary.items():\n",
    "        table_data.append([section, '', ''])\n",
    "        table_data.append(['‚îÄ' * 30, '‚îÄ' * 20, '‚îÄ' * 10])\n",
    "        for key, value in metrics.items():\n",
    "            table_data.append(['  ' + key, str(value), ''])\n",
    "        table_data.append(['', '', ''])\n",
    "    \n",
    "    # Create table\n",
    "    table = ax.table(cellText=table_data, cellLoc='left', loc='center',\n",
    "                    colWidths=[0.5, 0.3, 0.2])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Style header rows\n",
    "    for i, row in enumerate(table_data):\n",
    "        if row[1] == '':\n",
    "            for j in range(3):\n",
    "                cell = table[(i, j)]\n",
    "                cell.set_facecolor('#3498db')\n",
    "                cell.set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    plt.title('Training & Evaluation Summary', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    save_path = save_dir / 'summary_table.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_all_plots(history_path, comparison_results, save_dir='figures'):\n",
    "    \"\"\"\n",
    "    Generate t·∫•t c·∫£ plots c√πng l√∫c.\n",
    "    \n",
    "    Args:\n",
    "        history_path: str - Path to training_history.json\n",
    "        comparison_results: dict - Results t·ª´ evaluation\n",
    "        save_dir: str - Directory ƒë·ªÉ l∆∞u figures\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    plot_training_curves(history_path, save_dir)\n",
    "    plot_metrics_comparison(comparison_results, save_dir)\n",
    "    plot_loss_histogram(history_path, save_dir)\n",
    "    create_summary_table(comparison_results, history_path, save_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ ALL VISUALIZATIONS GENERATED!\")\n",
    "    print(f\"üìÅ Saved to: {save_dir}/\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    # ==================== CONFIGURATION ====================\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nüñ•Ô∏è  Device: {device}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    CONFIG = {\n",
    "        'src': SRC,\n",
    "        'trg': TRG,\n",
    "        'use_subword': USE_SUBWORD,\n",
    "        'use_rope': USE_ROPE,\n",
    "        'vocab_size_en': VOCAB_SIZE_EN,\n",
    "        'vocab_size_vi': VOCAB_SIZE_VI,\n",
    "        'vocab_model_type': VOCAB_MODEL_TYPE,\n",
    "        'num_workers': NUM_WORKERS,\n",
    "        # Model\n",
    "        'model_dim': MODEL_DIM,\n",
    "        'num_heads': NUM_HEADS,\n",
    "        'num_enc_layers': NUM_ENC_LAYERS,\n",
    "        'num_dec_layers': NUM_DEC_LAYERS,\n",
    "        'ff_hidden_dim': MODEL_DIM * 4,\n",
    "        'dropout': DROPOUT,\n",
    "        'max_len_en': MAX_LEN_EN,\n",
    "        'max_len_vi': MAX_LEN_VI,\n",
    "        \n",
    "        # Training\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        # 'learning_rate': 5e-5,  # Reduced to prevent NaN loss\n",
    "        'weight_decay': 1e-5,\n",
    "        'warmup_steps': WARMUP_STEPS,  # Warmup for stable training\n",
    "        'patience': 5,  # Early stopping\n",
    "        \n",
    "        # Data\n",
    "        'freq_threshold': 2,  # Minimum word frequency\n",
    "        # 'train_split': 'train[:80%]',\n",
    "        # 'val_split': 'train[80%:90%]',\n",
    "        # 'test_split': 'train[90%:]',\n",
    "        \n",
    "        # Inference\n",
    "        'beam_size': 5,\n",
    "        'length_penalty': 0.6,\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚öôÔ∏è  CONFIGURATION\")\n",
    "    print(\"=\"*60)\n",
    "    for key, value in CONFIG.items():\n",
    "        print(f\"  {key:<20}: {value}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # ==================== 1. LOAD DATA ====================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üì• LOADING IWSLT2015 DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    dataset = load_dataset('thainq107/iwslt2015-en-vi')\n",
    "    train_dataset = dataset['train']\n",
    "    val_dataset = dataset['validation']\n",
    "    test_dataset = dataset['test']\n",
    "    \n",
    "    train_dataset = preprocess_dataset(train_dataset)\n",
    "    val_dataset = preprocess_dataset(val_dataset)\n",
    "    test_dataset = preprocess_dataset(test_dataset, ignore=CONFIG['trg'])\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_dataset):,}\")\n",
    "    print(f\"  Val samples:   {len(val_dataset):,}\")\n",
    "    print(f\"  Test samples:  {len(test_dataset):,}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # ==================== 2. BUILD VOCABULARY ====================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìö BUILDING VOCABULARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    with open(\"temp_train.en\", \"w\", encoding=\"utf-8\") as f_en, \\\n",
    "     open(\"temp_train.vi\", \"w\", encoding=\"utf-8\") as f_vi:\n",
    "        for x in train_dataset:\n",
    "            f_en.write(x[\"en\"].strip() + \"\\n\")\n",
    "            f_vi.write(x[\"vi\"].strip() + \"\\n\")\n",
    "    \n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=\"temp_train.en\",\n",
    "        model_prefix=\"spm_en\",\n",
    "        vocab_size=CONFIG['vocab_size_en'],\n",
    "        model_type=CONFIG['vocab_model_type'],\n",
    "        character_coverage=1.0,\n",
    "        pad_id=0, bos_id=1, eos_id=2, unk_id=3\n",
    "    )\n",
    "\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=\"temp_train.vi\",\n",
    "        model_prefix=\"spm_vi\",\n",
    "        vocab_size=CONFIG['vocab_size_vi'],\n",
    "        model_type=CONFIG['vocab_model_type'],\n",
    "        character_coverage=0.9995,\n",
    "        pad_id=0, bos_id=1, eos_id=2, unk_id=3\n",
    "    )\n",
    "    os.remove(\"temp_train.en\")\n",
    "    os.remove(\"temp_train.vi\")\n",
    "    \n",
    "    src_sentences = [x[CONFIG['src']] for x in train_dataset]\n",
    "    trg_sentences = [x[CONFIG['trg']] for x in train_dataset]\n",
    "    \n",
    "    if CONFIG['use_subword']:\n",
    "        src_vocab = SubwordVocabulary(f\"spm_{CONFIG['src']}.model\")\n",
    "        trg_vocab = SubwordVocabulary(f\"spm_{CONFIG['trg']}.model\")\n",
    "    else:\n",
    "        src_vocab = Vocabulary(freq_threshold=CONFIG['freq_threshold'])\n",
    "        src_vocab.build_vocabulary(src_sentences)\n",
    "        \n",
    "        trg_vocab = Vocabulary(freq_threshold=CONFIG['freq_threshold'])\n",
    "        trg_vocab.build_vocabulary(trg_sentences)\n",
    "    \n",
    "    \n",
    "    # ==================== 3. CREATE DATALOADERS ====================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîÑ CREATING DATALOADERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if CONFIG['use_subword']:\n",
    "        pad_idx = src_vocab.pad_idx\n",
    "        train_data = SpmBilingualDataset(train_dataset, src_vocab, trg_vocab, src_lang=CONFIG['src'], trg_lang=CONFIG['trg'])\n",
    "        val_data = SpmBilingualDataset(val_dataset, src_vocab, trg_vocab, src_lang=CONFIG['src'], trg_lang=CONFIG['trg'])\n",
    "        test_data = SpmBilingualDataset(test_dataset, src_vocab, trg_vocab, src_lang=CONFIG['src'], trg_lang=CONFIG['trg'])\n",
    "    else:\n",
    "        pad_idx = src_vocab.stoi[PAD_TOKEN]\n",
    "        \n",
    "        train_data = BilingualDataset(train_dataset, src_vocab, trg_vocab, src_lang=CONFIG['src'], trg_lang=CONFIG['trg'])\n",
    "        val_data = BilingualDataset(val_dataset, src_vocab, trg_vocab, src_lang=CONFIG['src'], trg_lang=CONFIG['trg'])\n",
    "        test_data = BilingualDataset(test_dataset, src_vocab, trg_vocab, src_lang=CONFIG['src'], trg_lang=CONFIG['trg'])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=Collate(\n",
    "            pad_idx=pad_idx, \n",
    "            max_src_len=CONFIG[f\"max_len_{CONFIG['src']}\"],\n",
    "            max_trg_len=CONFIG[f\"max_len_{CONFIG['trg']}\"]\n",
    "        ),\n",
    "        num_workers=CONFIG['num_workers']\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=Collate(\n",
    "            pad_idx=pad_idx, \n",
    "            max_src_len=CONFIG[f\"max_len_{CONFIG['src']}\"],\n",
    "            max_trg_len=CONFIG[f\"max_len_{CONFIG['trg']}\"]\n",
    "        ),\n",
    "        num_workers=CONFIG['num_workers']\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=Collate(\n",
    "            pad_idx=pad_idx, \n",
    "            max_src_len=CONFIG[f\"max_len_{CONFIG['src']}\"],\n",
    "            max_trg_len=CONFIG[f\"max_len_{CONFIG['trg']}\"]\n",
    "        ),\n",
    "        num_workers=CONFIG['num_workers']\n",
    "    )\n",
    "    \n",
    "    print(f\"  Train batches: {len(train_loader)}\")\n",
    "    print(f\"  Val batches:   {len(val_loader)}\")\n",
    "    print(f\"  Test batches:  {len(test_loader)}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # ==================== 4. CREATE MODEL ====================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèóÔ∏è  CREATING TRANSFORMER MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model = Transformer(\n",
    "        src_vocab_size=len(src_vocab),\n",
    "        tgt_vocab_size=len(trg_vocab),\n",
    "        model_dim=CONFIG['model_dim'],\n",
    "        num_heads=CONFIG['num_heads'],\n",
    "        num_enc_layers=CONFIG['num_enc_layers'],\n",
    "        num_dec_layers=CONFIG['num_dec_layers'],\n",
    "        ff_hidden_dim=CONFIG['ff_hidden_dim'],\n",
    "        max_len_src=CONFIG[f\"max_len_{CONFIG['src']}\"],\n",
    "        max_len_trg=CONFIG[f\"max_len_{CONFIG['trg']}\"],\n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"  Total parameters:     {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # ==================== 5. SETUP TRAINING ====================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ SETUP TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Loss function (ignore padding)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx, label_smoothing=0.05)\n",
    "    \n",
    "    # Optimizer (learning rate s·∫Ω ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh b·ªüi warmup scheduler)\n",
    "    optimizer = create_optimizer(\n",
    "        model,\n",
    "        learning_rate=1.0,  # Base LR, s·∫Ω ƒë∆∞·ª£c warmup scheduler ƒëi·ªÅu ch·ªânh\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Warmup scheduler (theo paper \"Attention is All You Need\")\n",
    "    warmup_scheduler = WarmupScheduler(\n",
    "        optimizer,\n",
    "        d_model=CONFIG['model_dim'],\n",
    "        warmup_steps=CONFIG['warmup_steps']\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler (sau warmup)\n",
    "    plateau_scheduler = create_scheduler(\n",
    "        optimizer,\n",
    "        mode='plateau',\n",
    "        factor=0.5,\n",
    "        patience=3\n",
    "    )\n",
    "    \n",
    "    print(f\"  Loss function: CrossEntropyLoss (ignore_index={pad_idx}, label_smoothing=0.05)\")\n",
    "    print(f\"  Optimizer: Adam (base_lr=1.0, weight_decay={CONFIG['weight_decay']})\")\n",
    "    print(f\"  Warmup Scheduler: {CONFIG['warmup_steps']} steps\")\n",
    "    print(f\"  Plateau Scheduler: ReduceLROnPlateau (factor=0.5, patience=3)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # ==================== 6. TRAINING ====================\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        pad_idx=pad_idx,\n",
    "        checkpoint_dir='checkpoints',\n",
    "        log_dir='logs'\n",
    "    )\n",
    "    \n",
    "    trainer.train(\n",
    "        num_epochs=CONFIG['num_epochs'],\n",
    "        warmup_scheduler=warmup_scheduler,\n",
    "        plateau_scheduler=plateau_scheduler,\n",
    "        patience=CONFIG['patience']\n",
    "    )\n",
    "    \n",
    "    # ==================== 7. LOAD BEST MODEL ====================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üì¶ LOADING BEST MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    checkpoint = torch.load('checkpoints/best_model.pt', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"  Best epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"  Best val loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # ==================== 8. INFERENCE & EVALUATION ====================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç INFERENCE & EVALUATION\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Create decoders\n",
    "    greedy_decoder = GreedySearchDecoder(model, max_len=100, use_subword=CONFIG['use_subword'])\n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        model,\n",
    "        beam_size=CONFIG['beam_size'],\n",
    "        max_len=CONFIG[f\"max_len_{CONFIG['trg']}\"],\n",
    "        length_penalty=CONFIG['length_penalty'],\n",
    "        use_subword=CONFIG['use_subword']\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = Evaluator(model, test_loader, src_vocab, trg_vocab, device, use_subword=CONFIG['use_subword'])\n",
    "    comparison_results = evaluator.compare_decoders(greedy_decoder, beam_decoder)\n",
    "    \n",
    "    # ==================== 9. VISUALIZATION ====================\n",
    "    \n",
    "    generate_all_plots(\n",
    "        history_path='logs/training_history.json',\n",
    "        comparison_results=comparison_results,\n",
    "        save_dir='figures'\n",
    "    )\n",
    "    \n",
    "    # ==================== DONE ====================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ ALL TASKS COMPLETED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüìÅ Output files:\")\n",
    "    print(\"  - checkpoints/best_model.pt\")\n",
    "    print(\"  - logs/training_history.json\")\n",
    "    print(\"  - figures/training_curves.png\")\n",
    "    print(\"  - figures/metrics_comparison.png\")\n",
    "    print(\"  - figures/loss_histogram.png\")\n",
    "    print(\"  - figures/summary_table.png\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3411afd1",
   "metadata": {},
   "source": [
    "# Start training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
